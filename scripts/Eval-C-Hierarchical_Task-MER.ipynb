{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_path = \"../utils/\"\n",
    "corpus_path = \"../datasets/cantemist_v6/\"\n",
    "sub_task_path = \"cantemist-norm/\"\n",
    "test_gs_path = corpus_path + \"test-set/\" + sub_task_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 08:51:26.212789: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Auxiliary components\n",
    "import sys\n",
    "sys.path.insert(0, utils_path)\n",
    "from nlp_utils import *\n",
    "\n",
    "RES_DIR = \"../results/Cantemist/final_exec/\"\n",
    "\n",
    "# GS data\n",
    "df_test_gs = format_ner_gs(test_gs_path, subtask='norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ner_norm_performance(model_name, arr_execs):\n",
    "    \"\"\"\n",
    "    Sanity-check procedure that prints the NER performance of each single model execution.\n",
    "    \"\"\"\n",
    "    for i_exec in arr_execs:\n",
    "        print(\"Exec \" + str(i_exec) + \":\")\n",
    "        df_test_preds = pd.read_csv(RES_DIR + \"df_test_preds_c_hier_task_iob_\" + \\\n",
    "                str(model_name) + \"_\" + str(i_exec) + \".csv\", header=0, sep='\\t')\n",
    "        print(\"NER performance:\", calculate_ner_metrics(\n",
    "            gs=df_test_gs, \n",
    "            pred=format_ner_pred_df(\n",
    "                gs_path=test_gs_path, \n",
    "                df_preds=df_test_preds, \n",
    "                subtask='ner'\n",
    "            ),\n",
    "            subtask='ner'\n",
    "        ), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(dict_names_execs, \n",
    "                      df_gs=df_test_gs, path_gs=test_gs_path,\n",
    "                      round_n=3):\n",
    "    \"\"\"\n",
    "    Generate a pd.DataFrame with the statistics of the performance of each model.\n",
    "    \n",
    "    dict_names_execs: each key is a string with the model name, and \n",
    "                      each value is a list with the execs of the corresponding model.\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    for model_name in dict_names_execs:\n",
    "        p_res, r_res, f1_res = [], [], []\n",
    "        for i_exec in dict_names_execs[model_name]:\n",
    "            df_test_preds = pd.read_csv(RES_DIR + \"df_test_preds_c_hier_task_iob_\" + \\\n",
    "                    str(model_name) + \"_\" + str(i_exec) + \".csv\", header=0, sep='\\t')\n",
    "            p, r, f1 = calculate_ner_metrics(\n",
    "                gs=df_gs, \n",
    "                pred=format_ner_pred_df(\n",
    "                    gs_path=path_gs, \n",
    "                    df_preds=df_test_preds, \n",
    "                    subtask='ner'\n",
    "                ),\n",
    "                subtask='ner'\n",
    "            )\n",
    "            p_res.append(p)\n",
    "            r_res.append(r)\n",
    "            f1_res.append(f1)\n",
    "        p_res_stat = pd.Series(p_res).describe()\n",
    "        r_res_stat = pd.Series(r_res).describe()\n",
    "        f1_res_stat = pd.Series(f1_res).describe()\n",
    "        res_dict[model_name] = {\"P_avg\": round(p_res_stat['mean'], round_n), \"P_std\": round(p_res_stat['std'], round_n), \n",
    "                                \"P_max\": round(p_res_stat['max'], round_n),\n",
    "                                \"R_avg\": round(r_res_stat['mean'], round_n), \"R_std\": round(r_res_stat['std'], round_n), \n",
    "                                \"R_max\": round(r_res_stat['max'], round_n),\n",
    "                                \"F1_avg\": round(f1_res_stat['mean'], round_n), \"F1_std\": round(f1_res_stat['std'], round_n), \n",
    "                                \"F1_max\": round(f1_res_stat['max'], round_n)}\n",
    "    return pd.DataFrame(res_dict, index=[\"P_avg\", \"P_std\", \"P_max\", \n",
    "                                         \"R_avg\", \"R_std\", \"R_max\", \n",
    "                                         \"F1_avg\", \"F1_std\", \"F1_max\"]).transpose()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_paper(df_res):\n",
    "    arr_metrics = [\"P\", \"R\", \"F1\"]\n",
    "    arr_cols = []\n",
    "    for metric in arr_metrics:\n",
    "        df_res[metric + '_avg_std'] = df_res.apply(\n",
    "            lambda x: \".\" + str(x[metric + '_avg']).split('.')[-1] + \" ± \" + \\\n",
    "                \".\" + str(x[metric + '_std']).split('.')[-1], \n",
    "            axis=1\n",
    "        )\n",
    "        df_res[metric + '_max'] = df_res[metric + '_max'].apply(\n",
    "            lambda x: \".\" + str(x).split('.')[-1]\n",
    "        )\n",
    "        arr_cols += [metric + '_avg_std', metric + '_max']\n",
    "    return df_res[arr_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec 1:\n",
      "NER performance: (0.8627, 0.8632, 0.863)\n",
      "\n",
      "Exec 2:\n",
      "NER performance: (0.8587, 0.8662, 0.8624)\n",
      "\n",
      "Exec 3:\n",
      "NER performance: (0.8607, 0.8574, 0.8591)\n",
      "\n",
      "Exec 4:\n",
      "NER performance: (0.852, 0.8602, 0.856)\n",
      "\n",
      "Exec 5:\n",
      "NER performance: (0.8562, 0.8602, 0.8582)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m_name = \"xlmr\"\n",
    "execs = [1, 2, 3, 4, 5]\n",
    "\n",
    "check_ner_norm_performance(model_name=m_name, arr_execs=execs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec 1:\n",
      "NER performance: (0.8678, 0.8637, 0.8658)\n",
      "\n",
      "Exec 2:\n",
      "NER performance: (0.8752, 0.8571, 0.8661)\n",
      "\n",
      "Exec 3:\n",
      "NER performance: (0.8733, 0.8726, 0.8729)\n",
      "\n",
      "Exec 4:\n",
      "NER performance: (0.8573, 0.8745, 0.8658)\n",
      "\n",
      "Exec 5:\n",
      "NER performance: (0.8699, 0.8671, 0.8685)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m_name = \"xlmr_galen\"\n",
    "execs = [1, 2, 3, 4, 5]\n",
    "\n",
    "check_ner_norm_performance(model_name=m_name, arr_execs=execs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_avg</th>\n",
       "      <th>P_std</th>\n",
       "      <th>P_max</th>\n",
       "      <th>R_avg</th>\n",
       "      <th>R_std</th>\n",
       "      <th>R_max</th>\n",
       "      <th>F1_avg</th>\n",
       "      <th>F1_std</th>\n",
       "      <th>F1_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beto</th>\n",
       "      <td>0.862</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_galen</th>\n",
       "      <td>0.872</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert</th>\n",
       "      <td>0.862</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert_galen</th>\n",
       "      <td>0.871</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlmr</th>\n",
       "      <td>0.858</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlmr_galen</th>\n",
       "      <td>0.869</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             P_avg  P_std  P_max  R_avg  R_std  R_max  F1_avg  F1_std  F1_max\n",
       "beto         0.862  0.007  0.871  0.855  0.004  0.860   0.859   0.004   0.864\n",
       "beto_galen   0.872  0.005  0.879  0.868  0.005  0.872   0.870   0.004   0.875\n",
       "mbert        0.862  0.011  0.875  0.860  0.010  0.872   0.861   0.003   0.865\n",
       "mbert_galen  0.871  0.007  0.883  0.868  0.004  0.873   0.870   0.003   0.876\n",
       "xlmr         0.858  0.004  0.863  0.861  0.003  0.866   0.860   0.003   0.863\n",
       "xlmr_galen   0.869  0.007  0.875  0.867  0.007  0.874   0.868   0.003   0.873"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance(\n",
    "    {\n",
    "        'beto': [1, 2, 3, 4, 5], \n",
    "        'beto_galen': [1, 2, 3, 4, 5],\n",
    "        'mbert': [1, 2, 3, 4, 5], \n",
    "        'mbert_galen': [1, 2, 3, 4, 5],\n",
    "        'xlmr': [1, 2, 3, 4, 5], \n",
    "        'xlmr_galen': [1, 2, 3, 4, 5]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_avg_std</th>\n",
       "      <th>P_max</th>\n",
       "      <th>R_avg_std</th>\n",
       "      <th>R_max</th>\n",
       "      <th>F1_avg_std</th>\n",
       "      <th>F1_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beto</th>\n",
       "      <td>.862 ± .007</td>\n",
       "      <td>.871</td>\n",
       "      <td>.855 ± .004</td>\n",
       "      <td>.86</td>\n",
       "      <td>.859 ± .004</td>\n",
       "      <td>.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_galen</th>\n",
       "      <td>.872 ± .005</td>\n",
       "      <td>.879</td>\n",
       "      <td>.868 ± .005</td>\n",
       "      <td>.872</td>\n",
       "      <td>.87 ± .004</td>\n",
       "      <td>.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert</th>\n",
       "      <td>.862 ± .011</td>\n",
       "      <td>.875</td>\n",
       "      <td>.86 ± .01</td>\n",
       "      <td>.872</td>\n",
       "      <td>.861 ± .003</td>\n",
       "      <td>.865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert_galen</th>\n",
       "      <td>.871 ± .007</td>\n",
       "      <td>.883</td>\n",
       "      <td>.868 ± .004</td>\n",
       "      <td>.873</td>\n",
       "      <td>.87 ± .003</td>\n",
       "      <td>.876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlmr</th>\n",
       "      <td>.858 ± .004</td>\n",
       "      <td>.863</td>\n",
       "      <td>.861 ± .003</td>\n",
       "      <td>.866</td>\n",
       "      <td>.86 ± .003</td>\n",
       "      <td>.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlmr_galen</th>\n",
       "      <td>.869 ± .007</td>\n",
       "      <td>.875</td>\n",
       "      <td>.867 ± .007</td>\n",
       "      <td>.874</td>\n",
       "      <td>.868 ± .003</td>\n",
       "      <td>.873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               P_avg_std P_max    R_avg_std R_max   F1_avg_std F1_max\n",
       "beto         .862 ± .007  .871  .855 ± .004   .86  .859 ± .004   .864\n",
       "beto_galen   .872 ± .005  .879  .868 ± .005  .872   .87 ± .004   .875\n",
       "mbert        .862 ± .011  .875    .86 ± .01  .872  .861 ± .003   .865\n",
       "mbert_galen  .871 ± .007  .883  .868 ± .004  .873   .87 ± .003   .876\n",
       "xlmr         .858 ± .004  .863  .861 ± .003  .866   .86 ± .003   .863\n",
       "xlmr_galen   .869 ± .007  .875  .867 ± .007  .874  .868 ± .003   .873"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_df_paper(\n",
    "    model_performance(\n",
    "        {\n",
    "            'beto': [1, 2, 3, 4, 5], \n",
    "            'beto_galen': [1, 2, 3, 4, 5],\n",
    "            'mbert': [1, 2, 3, 4, 5], \n",
    "            'mbert_galen': [1, 2, 3, 4, 5],\n",
    "            'xlmr': [1, 2, 3, 4, 5], \n",
    "            'xlmr_galen': [1, 2, 3, 4, 5]\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the (F1) performance of all executions of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_f1_values(dict_names_execs, \n",
    "                    df_gs=df_test_gs, path_gs=test_gs_path):\n",
    "    \"\"\"\n",
    "    Generate a vector containing the F1 performance of all executions of all models, in the given order.\n",
    "    \n",
    "    dict_names_execs: each key is a string with the model name, and \n",
    "                      each value is a list with the random execs of the corresponding model.\n",
    "    \"\"\"\n",
    "    arr_values = []\n",
    "    for model_name in dict_names_execs:\n",
    "        for i_exec in dict_names_execs[model_name]:\n",
    "            df_test_preds = pd.read_csv(RES_DIR + \"df_test_preds_c_hier_task_iob_\" + \\\n",
    "                    str(model_name) + \"_\" + str(i_exec) + \".csv\", header=0, sep='\\t')\n",
    "            _, _, f1 = calculate_ner_metrics(\n",
    "                gs=df_gs, \n",
    "                pred=format_ner_pred_df(\n",
    "                    gs_path=path_gs, \n",
    "                    df_preds=df_test_preds, \n",
    "                    subtask='ner'\n",
    "                ),\n",
    "                subtask='ner'\n",
    "            )\n",
    "            arr_values.append(f1)\n",
    "    return arr_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_val = model_f1_values(\n",
    "    {\n",
    "        'beto': [1, 2, 3, 4, 5], \n",
    "        'beto_galen': [1, 2, 3, 4, 5],\n",
    "        'mbert': [1, 2, 3, 4, 5], \n",
    "        'mbert_galen': [1, 2, 3, 4, 5],\n",
    "        'xlmr': [1, 2, 3, 4, 5], \n",
    "        'xlmr_galen': [1, 2, 3, 4, 5]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(arr_val).to_csv(RES_DIR + \"ner_f1_exec_c_hier_task.csv\", index=False, header=False, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load texts from test corpus \n",
    "test_path = corpus_path + \"test-set/\" + sub_task_path\n",
    "test_files = [f for f in os.listdir(test_path) if os.path.isfile(test_path + f) and f.split('.')[-1] == 'txt']\n",
    "test_data = load_text_files(test_files, test_path)\n",
    "df_text_test = pd.DataFrame({'doc_id': [s.split('.txt')[0] for s in test_files], 'raw_text': test_data})\n",
    "\n",
    "# Load test doc list\n",
    "test_doc_list = sorted(set(df_text_test[\"doc_id\"]))\n",
    "\n",
    "iob_lab_decoder = {0: \"B\", 1: \"I\", 2: \"O\"}\n",
    "\n",
    "text_col = \"raw_text\"\n",
    "\n",
    "ENS_EVAL_STRAT = 'sum'\n",
    "RES_DIR_ENS = RES_DIR + \"ensemble/\"\n",
    "\n",
    "arr_exec = [1, 2, 3, 4, 5]\n",
    "model_exec = {\n",
    "    'mbert': arr_exec,\n",
    "    'mbert_galen': arr_exec,\n",
    "    'beto': arr_exec,\n",
    "    'beto_galen': arr_exec,\n",
    "    'xlmr': arr_exec,\n",
    "    'xlmr_galen': arr_exec\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def ens_performance(dict_names_execs, df_gs=df_test_gs, path_gs=test_gs_path, \n",
    "                        doc_list=test_doc_list, lab_decoder_list=[iob_lab_decoder],\n",
    "                        df_text=df_text_test, text_col=text_col, \n",
    "                        ens_eval_strategy=ENS_EVAL_STRAT):\n",
    "    ens_preds, ens_start_end = [], []\n",
    "    for model_name in dict_names_execs:\n",
    "        # Word start-send pairs\n",
    "        with open(RES_DIR_ENS + \"test_word_start_end_c_hier_task_iob_\" + str(model_name) + \".pck\", \"rb\") as f:\n",
    "            word_start_end = pickle.load(f)\n",
    "        for i_exec in dict_names_execs[model_name]:\n",
    "            # Word predictions\n",
    "            with open(RES_DIR_ENS + \"test_word_preds_c_hier_task_iob_\" + str(model_name) + \\\n",
    "                      \"_\" + str(i_exec) + \".pck\", \"rb\") as f:\n",
    "                ens_preds.append(pickle.load(f))\n",
    "                \n",
    "            ens_start_end.append(word_start_end)\n",
    "        \n",
    "    df_ens_ann = ens_ner_preds_brat_format(\n",
    "        doc_list=doc_list, \n",
    "        ens_doc_word_preds=ens_preds,                   \n",
    "        ens_doc_word_start_end=ens_start_end, \n",
    "        lab_decoder_list=lab_decoder_list, \n",
    "        df_text=df_text, \n",
    "        text_col=text_col, \n",
    "        ens_eval_strat=ens_eval_strategy,\n",
    "        subtask='ner'\n",
    "    )\n",
    "    return df_ens_ann, calculate_ner_metrics(\n",
    "        df_gs,\n",
    "        pred=format_ner_pred_df(\n",
    "            gs_path=path_gs, \n",
    "            df_preds=df_ens_ann, \n",
    "            subtask='ner'\n",
    "        ),\n",
    "        subtask='ner'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_model_name = ['beto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8792, 0.8712, 0.8752)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(dict_names_execs={x: model_exec[x] for x in arr_model_name})\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res['_'.join(arr_model_name)] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_iob_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BETO-Galén"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_model_name = ['beto_galen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8843, 0.8794, 0.8819)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(dict_names_execs={x: model_exec[x] for x in arr_model_name})\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res['_'.join(arr_model_name)] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_iob_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_model_name = ['mbert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8769, 0.8726, 0.8747)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(dict_names_execs={x: model_exec[x] for x in arr_model_name})\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res['_'.join(arr_model_name)] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_iob_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT-Galén"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_model_name = ['mbert_galen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8847, 0.8811, 0.8829)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(dict_names_execs={x: model_exec[x] for x in arr_model_name})\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res['_'.join(arr_model_name)] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_iob_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_model_name = ['xlmr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.874, 0.8723, 0.8731)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(dict_names_execs={x: model_exec[x] for x in arr_model_name})\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res['_'.join(arr_model_name)] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_iob_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R-Galén"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_model_name = ['xlmr_galen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8841, 0.8794, 0.8817)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(dict_names_execs={x: model_exec[x] for x in arr_model_name})\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res['_'.join(arr_model_name)] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_iob_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BETO + BETO-Galén"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_model_name = ['beto', 'beto_galen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8885, 0.8772, 0.8828)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(dict_names_execs={x: model_exec[x] for x in arr_model_name})\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res['_'.join(arr_model_name)] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_iob_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT + mBERT-Galén"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_model_name = ['mbert', 'mbert_galen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8856, 0.8803, 0.8829)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(dict_names_execs={x: model_exec[x] for x in arr_model_name})\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res['_'.join(arr_model_name)] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_iob_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R + XLM-R-Galén"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_model_name = ['xlmr', 'xlmr_galen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8855, 0.8816, 0.8836)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(dict_names_execs={x: model_exec[x] for x in arr_model_name})\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res['_'.join(arr_model_name)] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_iob_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BETO + mBERT + XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_model_name = ['beto', 'mbert', 'xlmr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8838, 0.8775, 0.8807)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(dict_names_execs={x: model_exec[x] for x in arr_model_name})\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res['_'.join(arr_model_name)] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_iob_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BETO-Galén + mBERT-Galén + XLM-R-Galén"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_model_name = ['beto_galen', 'mbert_galen', 'xlmr_galen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8917, 0.8863, 0.889)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(dict_names_execs={x: model_exec[x] for x in arr_model_name})\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res['_'.join(arr_model_name)] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_iob_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
