{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_path = \"../utils/\"\n",
    "corpus_path = \"../datasets/cantemist_v6/\"\n",
    "sub_task_path = \"cantemist-norm/\"\n",
    "test_gs_path = corpus_path + \"test-set/\" + sub_task_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 09:36:49.989280: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Auxiliary components\n",
    "import sys\n",
    "sys.path.insert(0, utils_path)\n",
    "from nlp_utils import *\n",
    "\n",
    "RES_DIR = \"../results/Cantemist/final_exec/\"\n",
    "\n",
    "# GS data\n",
    "df_test_gs = format_ner_gs(test_gs_path, subtask='norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ner_norm_performance(model_name, arr_execs):\n",
    "    \"\"\"\n",
    "    Sanity-check procedure that prints the NORM performance of each single model execution.\n",
    "    \"\"\"\n",
    "    for i_exec in arr_execs:\n",
    "        print(\"Exec \" + str(i_exec) + \":\")\n",
    "        df_test_preds = pd.read_csv(RES_DIR + \"df_test_preds_c_hier_task_cls_\" + \\\n",
    "                str(model_name) + \"_\" + str(i_exec) + \".csv\", header=0, sep='\\t')\n",
    "        print(\"NORM performance:\", calculate_ner_metrics(\n",
    "            gs=df_test_gs, \n",
    "            pred=format_ner_pred_df(\n",
    "                gs_path=test_gs_path, \n",
    "                df_preds=df_test_preds, \n",
    "                subtask='norm'\n",
    "            ),\n",
    "            subtask='norm'\n",
    "        ), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(dict_names_execs, \n",
    "                      df_gs=df_test_gs, path_gs=test_gs_path,\n",
    "                      round_n=3, multi_task=False):\n",
    "    \"\"\"\n",
    "    Generate a pd.DataFrame with the statistics of the performance of each model.\n",
    "    \n",
    "    dict_names_execs: each key is a string with the model name, and \n",
    "                      each value is a list with the execs of the corresponding model.\n",
    "    \"\"\"\n",
    "    res_dict = {}\n",
    "    for model_name in dict_names_execs:\n",
    "        p_res, r_res, f1_res = [], [], []\n",
    "        for i_exec in dict_names_execs[model_name]:\n",
    "            if multi_task:\n",
    "                df_test_preds = pd.read_csv(RES_DIR + \"df_test_preds_multi_task_ner_\" + str(i_exec) + \"_c_hier_task_cls_\" + \\\n",
    "                        str(model_name) + \"_\" + str(i_exec) + \".csv\", header=0, sep='\\t')\n",
    "            else:\n",
    "                df_test_preds = pd.read_csv(RES_DIR + \"df_test_preds_c_hier_task_cls_\" + \\\n",
    "                        str(model_name) + \"_\" + str(i_exec) + \".csv\", header=0, sep='\\t')\n",
    "            p, r, f1 = calculate_ner_metrics(\n",
    "                gs=df_gs, \n",
    "                pred=format_ner_pred_df(\n",
    "                    gs_path=path_gs, \n",
    "                    df_preds=df_test_preds, \n",
    "                    subtask='norm'\n",
    "                ),\n",
    "                subtask='norm'\n",
    "            )\n",
    "            p_res.append(p)\n",
    "            r_res.append(r)\n",
    "            f1_res.append(f1)\n",
    "        p_res_stat = pd.Series(p_res).describe()\n",
    "        r_res_stat = pd.Series(r_res).describe()\n",
    "        f1_res_stat = pd.Series(f1_res).describe()\n",
    "        res_dict[model_name] = {\"P_avg\": round(p_res_stat['mean'], round_n), \"P_std\": round(p_res_stat['std'], round_n), \n",
    "                                \"P_max\": round(p_res_stat['max'], round_n),\n",
    "                                \"R_avg\": round(r_res_stat['mean'], round_n), \"R_std\": round(r_res_stat['std'], round_n), \n",
    "                                \"R_max\": round(r_res_stat['max'], round_n),\n",
    "                                \"F1_avg\": round(f1_res_stat['mean'], round_n), \"F1_std\": round(f1_res_stat['std'], round_n), \n",
    "                                \"F1_max\": round(f1_res_stat['max'], round_n)}\n",
    "    return pd.DataFrame(res_dict, index=[\"P_avg\", \"P_std\", \"P_max\", \n",
    "                                         \"R_avg\", \"R_std\", \"R_max\", \n",
    "                                         \"F1_avg\", \"F1_std\", \"F1_max\"]).transpose()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_paper(df_res):\n",
    "    arr_metrics = [\"P\", \"R\", \"F1\"]\n",
    "    arr_cols = []\n",
    "    for metric in arr_metrics:\n",
    "        df_res[metric + '_avg_std'] = df_res.apply(\n",
    "            lambda x: \".\" + str(x[metric + '_avg']).split('.')[-1] + \" ± \" + \\\n",
    "                \".\" + str(x[metric + '_std']).split('.')[-1], \n",
    "            axis=1\n",
    "        )\n",
    "        df_res[metric + '_max'] = df_res[metric + '_max'].apply(\n",
    "            lambda x: \".\" + str(x).split('.')[-1]\n",
    "        )\n",
    "        arr_cols += [metric + '_avg_std', metric + '_max']\n",
    "    return df_res[arr_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec 1:\n",
      "NORM performance: (0.8215, 0.8219, 0.8217)\n",
      "\n",
      "Exec 2:\n",
      "NORM performance: (0.8131, 0.8203, 0.8167)\n",
      "\n",
      "Exec 3:\n",
      "NORM performance: (0.816, 0.8128, 0.8144)\n",
      "\n",
      "Exec 4:\n",
      "NORM performance: (0.8075, 0.8153, 0.8114)\n",
      "\n",
      "Exec 5:\n",
      "NORM performance: (0.8112, 0.815, 0.8131)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m_name = \"xlmr\"\n",
    "execs = [1, 2, 3, 4, 5]\n",
    "\n",
    "check_ner_norm_performance(model_name=m_name, arr_execs=execs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec 1:\n",
      "NORM performance: (0.8255, 0.8216, 0.8236)\n",
      "\n",
      "Exec 2:\n",
      "NORM performance: (0.8274, 0.8103, 0.8188)\n",
      "\n",
      "Exec 3:\n",
      "NORM performance: (0.8325, 0.8318, 0.8322)\n",
      "\n",
      "Exec 4:\n",
      "NORM performance: (0.8119, 0.8282, 0.82)\n",
      "\n",
      "Exec 5:\n",
      "NORM performance: (0.8219, 0.8192, 0.8205)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m_name = \"xlmr_galen\"\n",
    "execs = [1, 2, 3, 4, 5]\n",
    "\n",
    "check_ner_norm_performance(model_name=m_name, arr_execs=execs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_avg</th>\n",
       "      <th>P_std</th>\n",
       "      <th>P_max</th>\n",
       "      <th>R_avg</th>\n",
       "      <th>R_std</th>\n",
       "      <th>R_max</th>\n",
       "      <th>F1_avg</th>\n",
       "      <th>F1_std</th>\n",
       "      <th>F1_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beto</th>\n",
       "      <td>0.818</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_galen</th>\n",
       "      <td>0.825</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert</th>\n",
       "      <td>0.819</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert_galen</th>\n",
       "      <td>0.828</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlmr</th>\n",
       "      <td>0.814</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlmr_galen</th>\n",
       "      <td>0.824</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             P_avg  P_std  P_max  R_avg  R_std  R_max  F1_avg  F1_std  F1_max\n",
       "beto         0.818  0.007  0.824  0.811  0.004  0.816   0.814   0.003   0.819\n",
       "beto_galen   0.825  0.005  0.833  0.821  0.005  0.826   0.823   0.004   0.829\n",
       "mbert        0.819  0.009  0.832  0.818  0.009  0.830   0.818   0.002   0.820\n",
       "mbert_galen  0.828  0.007  0.839  0.825  0.004  0.830   0.826   0.004   0.832\n",
       "xlmr         0.814  0.005  0.822  0.817  0.004  0.822   0.815   0.004   0.822\n",
       "xlmr_galen   0.824  0.008  0.832  0.822  0.008  0.832   0.823   0.005   0.832"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance(\n",
    "    {\n",
    "        'beto': [1, 2, 3, 4, 5], \n",
    "        'beto_galen': [1, 2, 3, 4, 5],\n",
    "        'mbert': [1, 2, 3, 4, 5], \n",
    "        'mbert_galen': [1, 2, 3, 4, 5],\n",
    "        'xlmr': [1, 2, 3, 4, 5], \n",
    "        'xlmr_galen': [1, 2, 3, 4, 5]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_avg_std</th>\n",
       "      <th>P_max</th>\n",
       "      <th>R_avg_std</th>\n",
       "      <th>R_max</th>\n",
       "      <th>F1_avg_std</th>\n",
       "      <th>F1_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beto</th>\n",
       "      <td>.818 ± .007</td>\n",
       "      <td>.824</td>\n",
       "      <td>.811 ± .004</td>\n",
       "      <td>.816</td>\n",
       "      <td>.814 ± .003</td>\n",
       "      <td>.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_galen</th>\n",
       "      <td>.825 ± .005</td>\n",
       "      <td>.833</td>\n",
       "      <td>.821 ± .005</td>\n",
       "      <td>.826</td>\n",
       "      <td>.823 ± .004</td>\n",
       "      <td>.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert</th>\n",
       "      <td>.819 ± .009</td>\n",
       "      <td>.832</td>\n",
       "      <td>.818 ± .009</td>\n",
       "      <td>.83</td>\n",
       "      <td>.818 ± .002</td>\n",
       "      <td>.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert_galen</th>\n",
       "      <td>.828 ± .007</td>\n",
       "      <td>.839</td>\n",
       "      <td>.825 ± .004</td>\n",
       "      <td>.83</td>\n",
       "      <td>.826 ± .004</td>\n",
       "      <td>.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlmr</th>\n",
       "      <td>.814 ± .005</td>\n",
       "      <td>.822</td>\n",
       "      <td>.817 ± .004</td>\n",
       "      <td>.822</td>\n",
       "      <td>.815 ± .004</td>\n",
       "      <td>.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlmr_galen</th>\n",
       "      <td>.824 ± .008</td>\n",
       "      <td>.832</td>\n",
       "      <td>.822 ± .008</td>\n",
       "      <td>.832</td>\n",
       "      <td>.823 ± .005</td>\n",
       "      <td>.832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               P_avg_std P_max    R_avg_std R_max   F1_avg_std F1_max\n",
       "beto         .818 ± .007  .824  .811 ± .004  .816  .814 ± .003   .819\n",
       "beto_galen   .825 ± .005  .833  .821 ± .005  .826  .823 ± .004   .829\n",
       "mbert        .819 ± .009  .832  .818 ± .009   .83  .818 ± .002    .82\n",
       "mbert_galen  .828 ± .007  .839  .825 ± .004   .83  .826 ± .004   .832\n",
       "xlmr         .814 ± .005  .822  .817 ± .004  .822  .815 ± .004   .822\n",
       "xlmr_galen   .824 ± .008  .832  .822 ± .008  .832  .823 ± .005   .832"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_df_paper(\n",
    "    model_performance(\n",
    "        {\n",
    "            'beto': [1, 2, 3, 4, 5], \n",
    "            'beto_galen': [1, 2, 3, 4, 5],\n",
    "            'mbert': [1, 2, 3, 4, 5], \n",
    "            'mbert_galen': [1, 2, 3, 4, 5],\n",
    "            'xlmr': [1, 2, 3, 4, 5], \n",
    "            'xlmr_galen': [1, 2, 3, 4, 5]\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the (F1) performance of all executions of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_f1_values(dict_names_execs, \n",
    "                    df_gs=df_test_gs, path_gs=test_gs_path):\n",
    "    \"\"\"\n",
    "    Generate a vector containing the F1 performance of all executions of all models, in the given order.\n",
    "    \n",
    "    dict_names_execs: each key is a string with the model name, and \n",
    "                      each value is a list with the random execs of the corresponding model.\n",
    "    \"\"\"\n",
    "    arr_values = []\n",
    "    for model_name in dict_names_execs:\n",
    "        for i_exec in dict_names_execs[model_name]:\n",
    "            df_test_preds = pd.read_csv(RES_DIR + \"df_test_preds_c_hier_task_cls_\" + \\\n",
    "                    str(model_name) + \"_\" + str(i_exec) + \".csv\", header=0, sep='\\t')\n",
    "            _, _, f1 = calculate_ner_metrics(\n",
    "                gs=df_gs, \n",
    "                pred=format_ner_pred_df(\n",
    "                    gs_path=path_gs, \n",
    "                    df_preds=df_test_preds, \n",
    "                    subtask='norm'\n",
    "                ),\n",
    "                subtask='norm'\n",
    "            )\n",
    "            arr_values.append(f1)\n",
    "    return arr_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_val = model_f1_values(\n",
    "    {\n",
    "        'beto': [1, 2, 3, 4, 5], \n",
    "        'beto_galen': [1, 2, 3, 4, 5],\n",
    "        'mbert': [1, 2, 3, 4, 5], \n",
    "        'mbert_galen': [1, 2, 3, 4, 5],\n",
    "        'xlmr': [1, 2, 3, 4, 5], \n",
    "        'xlmr_galen': [1, 2, 3, 4, 5]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(arr_val).to_csv(RES_DIR + \"norm_f1_exec_c_hier_task.csv\", index=False, header=False, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = corpus_path + \"train-set/\" + sub_task_path\n",
    "train_files = [f for f in os.listdir(train_path) if os.path.isfile(train_path + f) and f.split('.')[-1] == \"txt\"]\n",
    "n_train_files = len(train_files)\n",
    "train_data = load_text_files(train_files, train_path)\n",
    "dev1_path = corpus_path + \"dev-set1/\" + sub_task_path\n",
    "train_files.extend([f for f in os.listdir(dev1_path) if os.path.isfile(dev1_path + f) and f.split('.')[-1] == \"txt\"])\n",
    "train_data.extend(load_text_files(train_files[n_train_files:], dev1_path))\n",
    "df_text_train = pd.DataFrame({'doc_id': [s.split('.txt')[0] for s in train_files], 'raw_text': train_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_path = corpus_path + \"dev-set2/\" + sub_task_path\n",
    "dev_files = [f for f in os.listdir(dev_path) if os.path.isfile(dev_path + f) and f.split('.')[-1] == \"txt\"]\n",
    "dev_data = load_text_files(dev_files, dev_path)\n",
    "df_text_dev = pd.DataFrame({'doc_id': [s.split('.txt')[0] for s in dev_files], 'raw_text': dev_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = corpus_path + \"test-set/\" + sub_task_path\n",
    "test_files = [f for f in os.listdir(test_path) if os.path.isfile(test_path + f) and f.split('.')[-1] == 'txt']\n",
    "test_data = load_text_files(test_files, test_path)\n",
    "df_text_test = pd.DataFrame({'doc_id': [s.split('.txt')[0] for s in test_files], 'raw_text': test_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ann_files = [train_path + f for f in os.listdir(train_path) if f.split('.')[-1] == \"ann\"]\n",
    "train_ann_files.extend([dev1_path + f for f in os.listdir(dev1_path) if f.split('.')[-1] == \"ann\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_codes_train_ner = process_brat_norm(train_ann_files).sort_values([\"doc_id\", \"start\", \"end\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_codes_train_ner[\"code_pre\"] = df_codes_train_ner[\"code\"].apply(lambda x: x.split('/')[0])\n",
    "df_codes_train_ner[\"code_suf\"] = df_codes_train_ner[\"code\"].apply(lambda x: '/'.join(x.split('/')[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ~df_codes_train_ner[[\"doc_id\", \"start\", \"end\"]].duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_codes_train_ner_final = df_codes_train_ner.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9737\n"
     ]
    }
   ],
   "source": [
    "print(df_codes_train_ner_final.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ann_files = [dev_path + f for f in os.listdir(dev_path) if f.split('.')[-1] == \"ann\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_codes_dev_ner = process_brat_norm(dev_ann_files).sort_values([\"doc_id\", \"start\", \"end\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_codes_dev_ner[\"code_pre\"] = df_codes_dev_ner[\"code\"].apply(lambda x: x.split('/')[0])\n",
    "df_codes_dev_ner[\"code_suf\"] = df_codes_dev_ner[\"code\"].apply(lambda x: '/'.join(x.split('/')[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ~df_codes_dev_ner[[\"doc_id\", \"start\", \"end\"]].duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_codes_dev_ner_final = df_codes_dev_ner.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2660\n"
     ]
    }
   ],
   "source": [
    "print(df_codes_dev_ner_final.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_codes_pre = sorted(set(df_codes_dev_ner_final[\"code_pre\"].values).union(set(\n",
    "    df_codes_train_ner_final[\"code_pre\"].values\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dev_codes_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_codes_suf = sorted(set(df_codes_dev_ner_final[\"code_suf\"].values).union(set(df_codes_train_ner_final[\"code_suf\"].values))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dev_codes_suf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code-pre\n",
    "code_pre_lab_encoder = {}\n",
    "code_pre_lab_decoder = {}\n",
    "i = 0\n",
    "for code in train_dev_codes_pre:\n",
    "    code_pre_lab_encoder[code] = i\n",
    "    code_pre_lab_decoder[i] = code\n",
    "    i += 1\n",
    "    \n",
    "code_pre_lab_encoder[\"O\"] = i\n",
    "code_pre_lab_decoder[i] = \"O\"\n",
    "\n",
    "# Code-suf\n",
    "code_suf_lab_encoder = {}\n",
    "code_suf_lab_decoder = {}\n",
    "i = 0\n",
    "for code in train_dev_codes_suf:\n",
    "    code_suf_lab_encoder[code] = i\n",
    "    code_suf_lab_decoder[i] = code\n",
    "    i += 1\n",
    "    \n",
    "code_suf_lab_encoder[\"O\"] = i\n",
    "code_suf_lab_decoder[i] = \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308 308\n"
     ]
    }
   ],
   "source": [
    "print(len(code_pre_lab_encoder), len(code_pre_lab_decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 36\n"
     ]
    }
   ],
   "source": [
    "print(len(code_suf_lab_encoder), len(code_suf_lab_decoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENS_EVAL_STRAT = 'sum'\n",
    "RES_DIR_ENS = RES_DIR + \"ensemble/\"\n",
    "\n",
    "subtask = 'norm'\n",
    "subtask_ann = subtask + '-iob_code_suf'\n",
    "\n",
    "arr_exec = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ens_performance(ens_name, ner_model_name, arr_model_name, \n",
    "                    res_dir=RES_DIR_ENS, prefix_name='', arr_exec=arr_exec, \n",
    "                    df_gs=df_test_gs, path_gs=test_gs_path, \n",
    "                    code_pre_lab_decoder=code_pre_lab_decoder, \n",
    "                    code_suf_lab_decoder=code_suf_lab_decoder, \n",
    "                    ens_eval_strategy=ENS_EVAL_STRAT, \n",
    "                    subtask=subtask, subtask_ann=subtask_ann):\n",
    "    df_ens_ner = pd.read_csv(res_dir + \"df_test_preds_ens_ner_\" + ens_name + \"_c_hier_task_cls_\" + \\\n",
    "                             ner_model_name + \"_ann.csv\", header=0, sep='\\t')\n",
    "    ens_preds_code_pre, ens_preds_code_suf = [], []\n",
    "    for model_name in arr_model_name:\n",
    "        for i_exec in arr_exec:\n",
    "            ens_preds_code_pre.append(np.load(file=res_dir + prefix_name + \"test_preds_code_pre_ens_ner_\" + ens_name + \\\n",
    "                                              \"_c_hier_task_cls_\" + model_name + \"_\" + str(i_exec) + \".npy\"))\n",
    "            ens_preds_code_suf.append(np.load(file=res_dir + prefix_name + \"test_preds_code_suf_ens_ner_\" + ens_name + \\\n",
    "                                              \"_c_hier_task_cls_\" + model_name + \"_\" + str(i_exec) + \".npy\"))\n",
    "\n",
    "    # Sanity check: all preds array have the same shape\n",
    "    assert len(ens_preds_code_pre) == len(ens_preds_code_suf)\n",
    "    for i in range(len(ens_preds_code_pre) - 1):\n",
    "        assert ens_preds_code_pre[i].shape == ens_preds_code_pre[i + 1].shape\n",
    "        assert ens_preds_code_suf[i].shape == ens_preds_code_suf[i + 1].shape\n",
    "    \n",
    "    print(\"Nº executions:\", len(ens_preds_code_pre))\n",
    "\n",
    "    if ens_eval_strategy == 'sum':\n",
    "        ens_code_pre = np.sum(ens_preds_code_pre, axis=0)\n",
    "        ens_code_suf = np.sum(ens_preds_code_suf, axis=0)\n",
    "    else: # default 'prod'\n",
    "        ens_code_pre = np.prod(ens_preds_code_pre, axis=0)\n",
    "        ens_code_suf = np.prod(ens_preds_code_suf, axis=0)\n",
    "\n",
    "    df_ens_preds = cls_code_norm_preds_brat_format(\n",
    "        y_pred_cls=[ens_code_pre, ens_code_suf], \n",
    "        df_pred_ner=df_ens_ner, \n",
    "        code_decoder_list=[code_pre_lab_decoder, code_suf_lab_decoder],\n",
    "        subtask=subtask_ann,\n",
    "        codes_pre_o_mask=None,\n",
    "        codes_pre_suf_mask=None\n",
    "    )\n",
    "\n",
    "    return df_ens_preds, calculate_ner_metrics(\n",
    "        gs=df_gs, \n",
    "        pred=format_ner_pred_df(\n",
    "            gs_path=path_gs, df_preds=df_ens_preds, \n",
    "            subtask=subtask\n",
    "        ),\n",
    "        subtask=subtask\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_name = 'beto'\n",
    "ner_model_name = ens_name\n",
    "arr_model_name = [ens_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº executions: 5\n",
      "(0.8358, 0.8282, 0.832)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(\n",
    "    ens_name=ens_name, \n",
    "    ner_model_name=ner_model_name, \n",
    "    arr_model_name=arr_model_name\n",
    ")\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res[ens_name] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_cls_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BETO-Galén"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_name = 'beto_galen'\n",
    "ner_model_name = 'beto_galen'\n",
    "arr_model_name = [ens_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº executions: 5\n",
      "(0.8384, 0.8337, 0.836)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(\n",
    "    ens_name=ens_name, \n",
    "    ner_model_name=ner_model_name, \n",
    "    arr_model_name=arr_model_name\n",
    ")\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res[ens_name] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_cls_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_name = 'mbert'\n",
    "ner_model_name = 'mbert'\n",
    "arr_model_name = [ens_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº executions: 5\n",
      "(0.8396, 0.8354, 0.8375)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(\n",
    "    ens_name=ens_name, \n",
    "    ner_model_name=ner_model_name, \n",
    "    arr_model_name=arr_model_name\n",
    ")\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res[ens_name] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_cls_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT-Galén"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_name = 'mbert_galen'\n",
    "ner_model_name = 'mbert_galen'\n",
    "arr_model_name = [ens_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº executions: 5\n",
      "(0.8433, 0.8398, 0.8415)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(\n",
    "    ens_name=ens_name, \n",
    "    ner_model_name=ner_model_name, \n",
    "    arr_model_name=arr_model_name\n",
    ")\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res[ens_name] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_cls_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_name = 'xlmr'\n",
    "ner_model_name = 'xlmr'\n",
    "arr_model_name = [ens_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº executions: 5\n",
      "(0.8348, 0.8332, 0.834)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(\n",
    "    ens_name=ens_name, \n",
    "    ner_model_name=ner_model_name, \n",
    "    arr_model_name=arr_model_name\n",
    ")\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res[ens_name] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_cls_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R-Galén"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_name = 'xlmr_galen'\n",
    "ner_model_name = 'xlmr_galen'\n",
    "arr_model_name = [ens_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº executions: 5\n",
      "(0.8426, 0.8382, 0.8403)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(\n",
    "    ens_name=ens_name, \n",
    "    ner_model_name=ner_model_name, \n",
    "    arr_model_name=arr_model_name\n",
    ")\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res[ens_name] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_cls_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BETO + BETO-Galén"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_model_name = ['beto', 'beto_galen']\n",
    "ens_name = '_'.join(arr_model_name)\n",
    "ner_model_name = 'beto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº executions: 10\n",
      "(0.8453, 0.8346, 0.8399)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(\n",
    "    ens_name=ens_name, \n",
    "    ner_model_name=ner_model_name, \n",
    "    arr_model_name=arr_model_name\n",
    ")\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res[ens_name] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_cls_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT + mBERT-Galén"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_model_name = ['mbert', 'mbert_galen']\n",
    "ens_name = '_'.join(arr_model_name)\n",
    "ner_model_name = 'mbert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº executions: 10\n",
      "(0.8463, 0.8412, 0.8437)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(\n",
    "    ens_name=ens_name, \n",
    "    ner_model_name=ner_model_name, \n",
    "    arr_model_name=arr_model_name\n",
    ")\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res[ens_name] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_cls_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R + XLM-R-Galén"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_model_name = ['xlmr', 'xlmr_galen']\n",
    "ens_name = '_'.join(arr_model_name)\n",
    "ner_model_name = 'xlmr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº executions: 10\n",
      "(0.8446, 0.8409, 0.8428)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(\n",
    "    ens_name=ens_name, \n",
    "    ner_model_name=ner_model_name, \n",
    "    arr_model_name=arr_model_name\n",
    ")\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res[ens_name] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_cls_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_ENS_PREF = \"multi_model_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_empty_ner_preds(ens_name, ner_model_name, arr_model_name, \n",
    "                        res_dir=RES_DIR_ENS, empty_value=0, \n",
    "                        prefix_name=MULTI_ENS_PREF):\n",
    "    \"\"\"\n",
    "    Considering a set of NER predictions as reference, this procedure inserts empty code-pre and code-suf\n",
    "    samples in the case a certain model does not predict a reference NER sample.\n",
    "    \"\"\"\n",
    "    # Load reference DF of NER predictions\n",
    "    df_ref_ens_ner = pd.read_csv(res_dir + \"df_test_preds_ens_ner_\" + ens_name + \"_c_hier_task_cls_\" + \\\n",
    "                                 ner_model_name + \"_ann.csv\", header=0, sep='\\t')\n",
    "    assert ~df_ref_ens_ner[['clinical_case', 'start', 'end']].duplicated().any()\n",
    "    for model_name in arr_model_name:\n",
    "        # Load DF of NER predictions from the current model\n",
    "        df_ens_ner = pd.read_csv(res_dir + \"df_test_preds_ens_ner_\" + ens_name + \"_c_hier_task_cls_\" + \\\n",
    "                                 model_name + \"_ann.csv\", header=0, sep='\\t')\n",
    "        assert df_ens_ner.shape[0] <= df_ref_ens_ner.shape[0]\n",
    "        arr_i_insert = []\n",
    "        if df_ens_ner.shape[0] < df_ref_ens_ner.shape[0]:\n",
    "            # Check the indices of the absent samples, i.e. reference NER samples that are not predicted by the current model\n",
    "            absent_samples = set(df_ref_ens_ner.apply(lambda x: str(x['clinical_case']) + ' ' + str(x['start']) + ' ' + str(x['end']), \n",
    "                                                      axis=1)) - \\\n",
    "                             set(df_ens_ner.apply(lambda x: str(x['clinical_case']) + ' ' + str(x['start']) + ' ' + str(x['end']), \n",
    "                                                  axis=1))\n",
    "            assert len(absent_samples) == df_ref_ens_ner.shape[0] - df_ens_ner.shape[0]\n",
    "            arr_i_absent = []\n",
    "            for s in absent_samples:\n",
    "                s = s.split(' ')\n",
    "                s_index = df_ref_ens_ner[(df_ref_ens_ner['clinical_case'] == s[0]) & (df_ref_ens_ner['start'] == int(s[1])) & \\\n",
    "                                         (df_ref_ens_ner['end'] == int(s[2]))].index[0]\n",
    "                arr_i_absent.append(np.where(df_ref_ens_ner.index.values == s_index)[0][0])\n",
    "            assert len(arr_i_absent) > 0\n",
    "            # Sort indices of absent samples\n",
    "            arr_i_absent = np.sort(arr_i_absent)\n",
    "            # Convert indices of absent samples to insertion indices\n",
    "            arr_i_insert.append(arr_i_absent[0])\n",
    "            for i in range(1, len(arr_i_absent)):\n",
    "                arr_i_insert.append(arr_i_absent[i] - i) # needs to be sanity checked\n",
    "        ## Insert empty values for the absent samples (if any)\n",
    "        for i_exec in arr_exec:\n",
    "            # Load predictions of both code prefix and suffix\n",
    "            preds_code_pre = np.load(file=res_dir + \"test_preds_code_pre_ens_ner_\" + ens_name + \\\n",
    "                                     \"_c_hier_task_cls_\" + model_name + \"_\" + str(i_exec) + \".npy\")\n",
    "            preds_code_suf = np.load(file=res_dir + \"test_preds_code_suf_ens_ner_\" + ens_name + \\\n",
    "                                     \"_c_hier_task_cls_\" + model_name + \"_\" + str(i_exec) + \".npy\")\n",
    "            preds_code_pre = np.insert(arr=preds_code_pre, obj=arr_i_insert, values=empty_value, axis=0)\n",
    "            preds_code_suf = np.insert(arr=preds_code_suf, obj=arr_i_insert, values=empty_value, axis=0)\n",
    "            # Save predictions after insertion\n",
    "            np.save(file=res_dir + prefix_name + \"test_preds_code_pre_ens_ner_\" + ens_name + \\\n",
    "                    \"_c_hier_task_cls_\" + model_name + \"_\" + str(i_exec) + \".npy\",\n",
    "                    arr=preds_code_pre)\n",
    "            np.save(file=res_dir + prefix_name + \"test_preds_code_suf_ens_ner_\" + ens_name + \\\n",
    "                    \"_c_hier_task_cls_\" + model_name + \"_\" + str(i_exec) + \".npy\",\n",
    "                    arr=preds_code_suf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BETO + mBERT + XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_name = \"beto_mbert_xlmr\"\n",
    "arr_model_name = ['beto', 'mbert', 'xlmr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_iob = pd.read_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_iob_\" + ens_name + \".csv\", header=0, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3607, 4)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ens_iob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beto NER predictions: 3607\n",
      "mbert NER predictions: 3607\n",
      "xlmr NER predictions: 3606\n"
     ]
    }
   ],
   "source": [
    "for model_name in arr_model_name:\n",
    "    df_ens_ner = pd.read_csv(RES_DIR_ENS + \"df_test_preds_ens_ner_\" + ens_name + \"_c_hier_task_cls_\" + \\\n",
    "                             model_name + \"_ann.csv\", header=0, sep='\\t')\n",
    "    print(model_name, \"NER predictions:\", df_ens_ner.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both BETO and mBERT models could be empoyed as NER-reference, since both make prediction for all NER samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model_name = \"beto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_empty_ner_preds(\n",
    "    ens_name=ens_name, \n",
    "    ner_model_name=ner_model_name, \n",
    "    arr_model_name=arr_model_name, \n",
    "    empty_value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº executions: 15\n",
      "(0.8453, 0.8393, 0.8423)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(\n",
    "    ens_name=ens_name, \n",
    "    ner_model_name=ner_model_name, \n",
    "    arr_model_name=arr_model_name,\n",
    "    prefix_name=MULTI_ENS_PREF\n",
    ")\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res[ens_name] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_cls_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BETO-Galén + mBERT-Galén + XLM-R-Galén"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_name = \"beto_galen_mbert_galen_xlmr_galen\"\n",
    "arr_model_name = ['beto_galen', 'mbert_galen', 'xlmr_galen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_iob = pd.read_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_iob_\" + ens_name + \".csv\", header=0, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3611, 4)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ens_iob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beto_galen NER predictions: 3610\n",
      "mbert_galen NER predictions: 3611\n",
      "xlmr_galen NER predictions: 3610\n"
     ]
    }
   ],
   "source": [
    "for model_name in arr_model_name:\n",
    "    df_ens_ner = pd.read_csv(RES_DIR_ENS + \"df_test_preds_ens_ner_\" + ens_name + \"_c_hier_task_cls_\" + \\\n",
    "                             model_name + \"_ann.csv\", header=0, sep='\\t')\n",
    "    print(model_name, \"NER predictions:\", df_ens_ner.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only mBERT-Galén models could be empoyed as NER-reference, since it makes prediction for all NER samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model_name = \"mbert_galen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_empty_ner_preds(\n",
    "    ens_name=ens_name, \n",
    "    ner_model_name=ner_model_name, \n",
    "    arr_model_name=arr_model_name, \n",
    "    empty_value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº executions: 15\n",
      "(0.8518, 0.8467, 0.8493)\n"
     ]
    }
   ],
   "source": [
    "df_ens_ann, res_metrics = ens_performance(\n",
    "    ens_name=ens_name, \n",
    "    ner_model_name=ner_model_name, \n",
    "    arr_model_name=arr_model_name,\n",
    "    prefix_name=MULTI_ENS_PREF\n",
    ")\n",
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ens_res[ens_name] = res_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_ann.to_csv(RES_DIR_ENS + \"df_test_preds_c_hier_task_cls_\" + '_'.join(arr_model_name) + \\\n",
    "                  \".csv\", index=False, header=True, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dic_ens_res, index=[\"P\", \"R\", \"F1\"]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beto</th>\n",
       "      <td>0.8358</td>\n",
       "      <td>0.8282</td>\n",
       "      <td>0.8320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_galen</th>\n",
       "      <td>0.8384</td>\n",
       "      <td>0.8337</td>\n",
       "      <td>0.8360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert</th>\n",
       "      <td>0.8396</td>\n",
       "      <td>0.8354</td>\n",
       "      <td>0.8375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert_galen</th>\n",
       "      <td>0.8433</td>\n",
       "      <td>0.8398</td>\n",
       "      <td>0.8415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlmr</th>\n",
       "      <td>0.8348</td>\n",
       "      <td>0.8332</td>\n",
       "      <td>0.8340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlmr_galen</th>\n",
       "      <td>0.8426</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.8403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_beto_galen</th>\n",
       "      <td>0.8453</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.8399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert_mbert_galen</th>\n",
       "      <td>0.8463</td>\n",
       "      <td>0.8412</td>\n",
       "      <td>0.8437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlmr_xlmr_galen</th>\n",
       "      <td>0.8446</td>\n",
       "      <td>0.8409</td>\n",
       "      <td>0.8428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_mbert_xlmr</th>\n",
       "      <td>0.8453</td>\n",
       "      <td>0.8393</td>\n",
       "      <td>0.8423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_galen_mbert_galen_xlmr_galen</th>\n",
       "      <td>0.8518</td>\n",
       "      <td>0.8467</td>\n",
       "      <td>0.8493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        P       R      F1\n",
       "beto                               0.8358  0.8282  0.8320\n",
       "beto_galen                         0.8384  0.8337  0.8360\n",
       "mbert                              0.8396  0.8354  0.8375\n",
       "mbert_galen                        0.8433  0.8398  0.8415\n",
       "xlmr                               0.8348  0.8332  0.8340\n",
       "xlmr_galen                         0.8426  0.8382  0.8403\n",
       "beto_beto_galen                    0.8453  0.8346  0.8399\n",
       "mbert_mbert_galen                  0.8463  0.8412  0.8437\n",
       "xlmr_xlmr_galen                    0.8446  0.8409  0.8428\n",
       "beto_mbert_xlmr                    0.8453  0.8393  0.8423\n",
       "beto_galen_mbert_galen_xlmr_galen  0.8518  0.8467  0.8493"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-task NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = format_df_paper(\n",
    "    model_performance(\n",
    "        {\n",
    "            'beto': [1, 2, 3, 4, 5], \n",
    "            'beto_galen': [1, 2, 3, 4, 5],\n",
    "            'mbert': [1, 2, 3, 4, 5], \n",
    "            'mbert_galen': [1, 2, 3, 4, 5],\n",
    "            'xlmr': [1, 2, 3, 4, 5], \n",
    "            'xlmr_galen': [1, 2, 3, 4, 5]\n",
    "        },\n",
    "        multi_task=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_avg_std</th>\n",
       "      <th>P_max</th>\n",
       "      <th>R_avg_std</th>\n",
       "      <th>R_max</th>\n",
       "      <th>F1_avg_std</th>\n",
       "      <th>F1_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beto</th>\n",
       "      <td>.814 ± .013</td>\n",
       "      <td>.834</td>\n",
       "      <td>.809 ± .005</td>\n",
       "      <td>.816</td>\n",
       "      <td>.812 ± .005</td>\n",
       "      <td>.818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_galen</th>\n",
       "      <td>.819 ± .005</td>\n",
       "      <td>.826</td>\n",
       "      <td>.82 ± .005</td>\n",
       "      <td>.825</td>\n",
       "      <td>.819 ± .002</td>\n",
       "      <td>.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert</th>\n",
       "      <td>.813 ± .006</td>\n",
       "      <td>.817</td>\n",
       "      <td>.817 ± .004</td>\n",
       "      <td>.821</td>\n",
       "      <td>.815 ± .002</td>\n",
       "      <td>.817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert_galen</th>\n",
       "      <td>.825 ± .012</td>\n",
       "      <td>.843</td>\n",
       "      <td>.824 ± .004</td>\n",
       "      <td>.83</td>\n",
       "      <td>.825 ± .005</td>\n",
       "      <td>.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlmr</th>\n",
       "      <td>.808 ± .003</td>\n",
       "      <td>.812</td>\n",
       "      <td>.812 ± .006</td>\n",
       "      <td>.821</td>\n",
       "      <td>.81 ± .002</td>\n",
       "      <td>.813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlmr_galen</th>\n",
       "      <td>.821 ± .008</td>\n",
       "      <td>.835</td>\n",
       "      <td>.821 ± .003</td>\n",
       "      <td>.825</td>\n",
       "      <td>.821 ± .004</td>\n",
       "      <td>.827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               P_avg_std P_max    R_avg_std R_max   F1_avg_std F1_max\n",
       "beto         .814 ± .013  .834  .809 ± .005  .816  .812 ± .005   .818\n",
       "beto_galen   .819 ± .005  .826   .82 ± .005  .825  .819 ± .002   .822\n",
       "mbert        .813 ± .006  .817  .817 ± .004  .821  .815 ± .002   .817\n",
       "mbert_galen  .825 ± .012  .843  .824 ± .004   .83  .825 ± .005   .832\n",
       "xlmr         .808 ± .003  .812  .812 ± .006  .821   .81 ± .002   .813\n",
       "xlmr_galen   .821 ± .008  .835  .821 ± .003  .825  .821 ± .004   .827"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the (F1) performance of all executions of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_model_f1_values(dict_names_execs, \n",
    "                    df_gs=df_test_gs, path_gs=test_gs_path):\n",
    "    \"\"\"\n",
    "    Generate a vector containing the F1 performance of all executions of all models, in the given order.\n",
    "    \n",
    "    dict_names_execs: each key is a string with the model name, and \n",
    "                      each value is a list with the random execs of the corresponding model.\n",
    "    \"\"\"\n",
    "    arr_values = []\n",
    "    for model_name in dict_names_execs:\n",
    "        for i_exec in dict_names_execs[model_name]:\n",
    "            df_test_preds = pd.read_csv(RES_DIR + \"df_test_preds_multi_task_ner_\" + str(i_exec) + \"_c_hier_task_cls_\" + \\\n",
    "                    str(model_name) + \"_\" + str(i_exec) + \".csv\", header=0, sep='\\t')\n",
    "            _, _, f1 = calculate_ner_metrics(\n",
    "                gs=df_gs, \n",
    "                pred=format_ner_pred_df(\n",
    "                    gs_path=path_gs, \n",
    "                    df_preds=df_test_preds, \n",
    "                    subtask='norm'\n",
    "                ),\n",
    "                subtask='norm'\n",
    "            )\n",
    "            arr_values.append(f1)\n",
    "    return arr_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_val = multi_model_f1_values(\n",
    "    {\n",
    "        'beto': [1, 2, 3, 4, 5], \n",
    "        'beto_galen': [1, 2, 3, 4, 5],\n",
    "        'mbert': [1, 2, 3, 4, 5], \n",
    "        'mbert_galen': [1, 2, 3, 4, 5],\n",
    "        'xlmr': [1, 2, 3, 4, 5], \n",
    "        'xlmr_galen': [1, 2, 3, 4, 5]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(arr_val).to_csv(RES_DIR + \"norm_f1_exec_c_multi_task_ner_hier_task.csv\", index=False, header=False, sep = '\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
